{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d3ca2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig ,DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "from transformers import AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61d268",
   "metadata": {},
   "source": [
    "# More Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_parquet('processed_data/relations_qa.parquet')\n",
    "pos_samples = samples[samples.answerability == 1]\n",
    "neg_samples = samples[samples.answerability == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bdfad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>note_question_id</th>\n",
       "      <th>sub_context</th>\n",
       "      <th>question</th>\n",
       "      <th>answerability</th>\n",
       "      <th>token_start</th>\n",
       "      <th>token_end</th>\n",
       "      <th>char_start</th>\n",
       "      <th>char_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0329</td>\n",
       "      <td>0</td>\n",
       "      <td>[2014-01-20, 10:46, PM, BLOOD, Glucose, -, 114...</td>\n",
       "      <td>[Has, the, patient, had, any, rll, pneumonia, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>80</td>\n",
       "      <td>262</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0329</td>\n",
       "      <td>1</td>\n",
       "      <td>[However, ,, when, asked, if, she, thinks, he,...</td>\n",
       "      <td>[Has, the, patient, had, any, mild, background...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>377</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0329</td>\n",
       "      <td>2</td>\n",
       "      <td>[His, family, was, notified, that, he, was, do...</td>\n",
       "      <td>[Has, the, patient, had, any, early, sepsis, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>334</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0329</td>\n",
       "      <td>3</td>\n",
       "      <td>[Physical, Exam, :, Vitals, :, T, 102.8, HR, 1...</td>\n",
       "      <td>[Has, the, patient, had, any, accessory, muscl...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>79</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0329</td>\n",
       "      <td>4</td>\n",
       "      <td>[However, ,, when, asked, if, she, thinks, he,...</td>\n",
       "      <td>[When, was, the, patient, evaluated, for, chf,...</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>96</td>\n",
       "      <td>465</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   note_id  note_question_id  \\\n",
       "4     0329                 0   \n",
       "7     0329                 1   \n",
       "17    0329                 2   \n",
       "20    0329                 3   \n",
       "25    0329                 4   \n",
       "\n",
       "                                          sub_context  \\\n",
       "4   [2014-01-20, 10:46, PM, BLOOD, Glucose, -, 114...   \n",
       "7   [However, ,, when, asked, if, she, thinks, he,...   \n",
       "17  [His, family, was, notified, that, he, was, do...   \n",
       "20  [Physical, Exam, :, Vitals, :, T, 102.8, HR, 1...   \n",
       "25  [However, ,, when, asked, if, she, thinks, he,...   \n",
       "\n",
       "                                             question  answerability  \\\n",
       "4   [Has, the, patient, had, any, rll, pneumonia, ...              1   \n",
       "7   [Has, the, patient, had, any, mild, background...              1   \n",
       "17  [Has, the, patient, had, any, early, sepsis, a...              1   \n",
       "20  [Has, the, patient, had, any, accessory, muscl...              1   \n",
       "25  [When, was, the, patient, evaluated, for, chf,...              1   \n",
       "\n",
       "    token_start  token_end  char_start  char_end  \n",
       "4            51         80         262       408  \n",
       "7            71         79         377       426  \n",
       "17           65         75         334       386  \n",
       "20           18         42          79       212  \n",
       "25           91         96         465       485  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04753345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pos 66584\n",
      "# neg 467926\n",
      "# neg 66584\n"
     ]
    }
   ],
   "source": [
    "# pos_samples = resample(pos_samples, replace=False, n_samples=100, random_state=0) \n",
    "print('# pos', len(pos_samples))\n",
    "print('# neg', len(neg_samples))\n",
    "neg_samples = resample(neg_samples, replace=False, n_samples=len(pos_samples), random_state=0) \n",
    "print('# neg', len(neg_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf946e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79900, 2) (79900, 5)\n",
      "(26634, 2) (26634, 5)\n",
      "(26634, 2) (26634, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "samples = pd.concat([pos_samples, neg_samples], axis=0)\n",
    "\n",
    "X = samples[['sub_context', 'question']]\n",
    "X['question'] = X['question'].apply(lambda x: ' '.join(x))\n",
    "X['sub_context'] = X['sub_context'].apply(lambda x: ' '.join(x))\n",
    "y = samples[['answerability', 'token_start', 'token_end', 'char_start', 'char_end']]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=0, \n",
    "                                                    stratify=np.asarray(y['answerability']))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                                  test_size=0.25, \n",
    "                                                  random_state=0,\n",
    "                                                  stratify=np.asarray(y_train['answerability']))\n",
    "    \n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729baa8",
   "metadata": {},
   "source": [
    "# Tokenizer + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ec6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bio = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edc173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_qa(X, y, filename=None):\n",
    "    encodings = tokenizer_bio.batch_encode_plus(\n",
    "      list(X.values), \n",
    "      return_tensors='pt',\n",
    "      add_special_tokens = True,\n",
    "      return_token_type_ids=True,\n",
    "      padding='max_length',\n",
    "      max_length=512,\n",
    "      return_attention_mask = True,\n",
    "      truncation='longest_first', \n",
    "      return_offsets_mapping=False\n",
    "    )\n",
    "    \n",
    "    embed_token_start = []\n",
    "    embed_token_end = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "   \n",
    "        # Start position, find closest left direction\n",
    "        start = encodings.char_to_token(i, y.char_start.iloc[i])\n",
    "        j=0\n",
    "        while start is None and j < 1000:\n",
    "            j+=1\n",
    "            start = encodings.char_to_token(i, max(0, y.char_start.iloc[i]-j))\n",
    "        j=0\n",
    "        while start is None and j < 1000:\n",
    "            j+=1\n",
    "            start = encodings.char_to_token(i, max(0, y.char_start.iloc[i]+j))\n",
    "        # 0 index\n",
    "        embed_token_start.append(start-1)\n",
    "        \n",
    "        \n",
    "        # End position, find closest right direction then left direction\n",
    "        end = encodings.char_to_token(i, max(0, y.char_end.iloc[i]-1))\n",
    "        j=0\n",
    "        while end is None and j < 1000:\n",
    "            j+=1\n",
    "            end = encodings.char_to_token(i, max(0, y.char_end.iloc[i]+j-1))\n",
    "        j=0\n",
    "        while end is None and j < 1000:\n",
    "            j+=1\n",
    "            end = encodings.char_to_token(i, max(0, y.char_end.iloc[i]-j-1))\n",
    "        # 0 index\n",
    "        embed_token_end.append(end-1)\n",
    "\n",
    "\n",
    "    encodings['start_positions'] = torch.tensor(embed_token_start)\n",
    "    encodings['end_positions'] = torch.tensor(embed_token_end)\n",
    "    \n",
    "    if filename:\n",
    "        with open(f'processed_data/{filename}.pickle', 'wb') as f:\n",
    "            pickle.dump(encodings, f)\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1359376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize_qa(X_train, y_train, filename='relations_qa_bio_train')\n",
    "val_encodings = tokenize_qa(X_val, y_val, filename='relations_qa_bio_val')\n",
    "test_encodings = tokenize_qa(X_test, y_test, filename='relations_qa_bio_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94374a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_encodings['end_positions']).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520e787",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9945cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ermQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        with open(f\"processed_data/{filename}.pickle\", \"rb\") as f:\n",
    "            self.encodings = pickle.load(f)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef828df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "metadata = dict()\n",
    "# for param in model.distilbert.parameters():\n",
    "#     param.requires_grad = False\n",
    "# model = torch.load('models/bioBERT/m_1_uf_e_6_vl_0.7865.model')\n",
    "# with open('models/bioBERT/m_1_uf_e_6_vl_0.7865_metadata.pickle', 'rb') as f:\n",
    "#     metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ef05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 6\n",
    "train_dataset = ermQA('relations_qa_bio_train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "TEST_BATCH_SIZE = 6\n",
    "val_dataset = ermQA('relations_qa_bio_val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=TEST_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c6313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13317 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 13317/13317 [2:21:23<00:00,  1.57it/s] \n",
      "  0%|          | 0/4439 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 4439/4439 [14:05<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 0.7206070639414297, val_loss: 0.4803323878521256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13317/13317 [2:19:22<00:00,  1.59it/s] \n",
      "100%|██████████| 4439/4439 [14:03<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_loss: 0.46262662184167275, val_loss: 0.45042239221997304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13317/13317 [2:17:02<00:00,  1.62it/s] \n",
      "100%|██████████| 4439/4439 [13:26<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_loss: 4.747472540038395, val_loss: 12.476650028783046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13317/13317 [2:15:52<00:00,  1.63it/s] \n",
      "100%|██████████| 4439/4439 [13:28<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train_loss: 12.479288038248198, val_loss: 12.476649371372952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 661/13317 [06:46<2:09:48,  1.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1924/3256008729.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "if metadata == {}:\n",
    "    START_EPOCH = 0\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "else:\n",
    "    START_EPOCH = metadata['epoch'] + 1\n",
    "    train_loss = metadata['train_loss']\n",
    "    val_loss = metadata['valid_loss']\n",
    "\n",
    "\n",
    "for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        token_type_ids = torch.tensor(batch['token_type_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        start_positions = torch.tensor(batch['start_positions']).to(device)\n",
    "        end_positions = torch.tensor(batch['end_positions']).to(device)\n",
    "        \n",
    "        outputs = model(input_ids = input_ids, \n",
    "                        token_type_ids = token_type_ids, \n",
    "                        attention_mask = attention_mask\n",
    "                       )  \n",
    "        loss = loss_func(outputs['start_logits'], start_positions) + loss_func(outputs['end_logits'], end_positions)\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "            token_type_ids = torch.tensor(batch['token_type_ids']).to(device)\n",
    "            attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "            start_positions = torch.tensor(batch['start_positions']).to(device)\n",
    "            end_positions = torch.tensor(batch['end_positions']).to(device)\n",
    "            outputs = model(input_ids = input_ids, \n",
    "                            token_type_ids = token_type_ids, \n",
    "                            attention_mask = attention_mask\n",
    "                           )  \n",
    "            loss = loss_func(outputs['start_logits'], start_positions) + loss_func(outputs['end_logits'], end_positions)\n",
    "            batch_loss.append(loss.item())\n",
    "    val_loss.append(np.mean(batch_loss))\n",
    "    \n",
    "    \n",
    "    print(f'Epoch: {epoch}, train_loss: {train_loss[-1]}, val_loss: {val_loss[-1]}')\n",
    "    \n",
    "    model_name = f'models/bioBERT/m_1_uf_e_{len(val_loss)}_vl_{round(val_loss[-1], 4)}'\n",
    "    metadata = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': val_loss\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Early Stopping\n",
    "    if len(val_loss) > 3:\n",
    "        if val_loss[-1] > val_loss[-2] > val_loss[-3]:\n",
    "            torch.save(model, f'{model_name}.model')\n",
    "            \n",
    "            with open(f'{model_name}_metadata.pickle', 'wb') as f:\n",
    "                pickle.dump(metadata, f)\n",
    "            break\n",
    "            \n",
    "    # Check point\n",
    "#     if len(val_loss) % 4 == 0 or NUM_EPOCHS == epoch+1:\n",
    "    torch.save(model, f'{model_name}.model ')\n",
    "    with open(f'{model_name}_metadata.pickle', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f20b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6dd31e",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524a1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(prediction, truth):\n",
    "    exact_match = 0\n",
    "    for pred, tru in zip(prediction, truth):\n",
    "        if pred == tru:\n",
    "            exact_match += 1\n",
    "    return exact_match / len(prediction)\n",
    "\n",
    "\n",
    "def getOverlap(a, b):\n",
    "    return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "\n",
    "\n",
    "def compute_tav_fast(start_logits, end_logits):\n",
    "    start_probs, end_probs = (torch.softmax(start_logits, dim=0), torch.softmax(end_logits, dim=0))\n",
    "    n = len(start_logits)\n",
    "    s_null = start_probs[0] + end_probs[0]\n",
    "    s_has = 0\n",
    "    best_span = (0, 0)\n",
    "    high_start_idx = 1\n",
    "  \n",
    "    for i in range(1, n):\n",
    "        if start_probs[i] > start_probs[high_start_idx]:\n",
    "            high_start_idx = i\n",
    "        if start_probs[high_start_idx] + end_probs[i] > s_has:\n",
    "            s_has = start_probs[high_start_idx] + end_probs[i]\n",
    "            best_span = (high_start_idx, i)\n",
    "            \n",
    "    s_diff = s_null - s_has\n",
    "    return s_diff.item(), best_span\n",
    "\n",
    "def compute_f1(prediction, truth, input_id_list):\n",
    "    \n",
    "    f1_scores = []\n",
    "    \n",
    "    for i, (pred, gold) in enumerate(zip(prediction, truth)):\n",
    "        \n",
    "        if pred == gold:\n",
    "            f1_scores.append(1)\n",
    "            \n",
    "        else:\n",
    "            pred_tokens = input_id_list[i-1][pred[0]: pred[1]+1]\n",
    "            gold_tokens = input_id_list[i-1][gold[0]: gold[1]+1]\n",
    "\n",
    "            total_overlap = len(set(pred_tokens).intersection(set(gold_tokens)))\n",
    "            total_tru = len(gold_tokens)\n",
    "            total_pred = len(pred_tokens)\n",
    "\n",
    "            if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "                f1_scores.append(int(pred_tokens == gold_tokens))\n",
    "            elif total_overlap == 0:\n",
    "                f1_scores.append(0)\n",
    "            else:\n",
    "                prec = total_overlap / total_pred\n",
    "                rec = total_overlap / total_tru\n",
    "                f1_scores.append(2 * (prec * rec) / (prec + rec))\n",
    "    \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "306c1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, data_loader, split):\n",
    "    truths = []\n",
    "    \n",
    "    # Greedy\n",
    "    spans_greedy = []\n",
    "    \n",
    "    # TAV\n",
    "    s_diffs = []\n",
    "    spans_tav = []\n",
    "    \n",
    "    input_id_list = []\n",
    "    \n",
    "    model.eval()\n",
    "    for batch in tqdm(data_loader):\n",
    "        \n",
    "        # we don't need to calculate gradients as we're not training\n",
    "        with torch.no_grad():\n",
    "            # Truth Span\n",
    "            for i, j, k in zip(batch['start_positions'], batch['end_positions'], batch['input_ids']):\n",
    "                truths.append((i.item(), j.item()))\n",
    "                input_id_list.append(k.tolist())\n",
    "        \n",
    "            \n",
    "            # Predictions\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Greedy\n",
    "            for i, j in zip(torch.argmax(outputs['start_logits'], dim=1), torch.argmax(outputs['end_logits'], dim=1)):\n",
    "                spans_greedy.append((i.item(), j.item()))\n",
    "            \n",
    "            # TAV\n",
    "            for i, j in zip(outputs['start_logits'], outputs['end_logits']):\n",
    "                s_diff, span = compute_tav_fast(i, j)\n",
    "                s_diffs.append(s_diff)\n",
    "                spans_tav.append(span)\n",
    "    \n",
    "    predictions_metadata = {\n",
    "        'truth': truths,\n",
    "        'spans_greedy': spans_greedy,\n",
    "        's_diffs': s_diffs,\n",
    "        'spans_tav': spans_tav,\n",
    "        'input_id_list': input_id_list\n",
    "    }\n",
    "    with open(f'predictions/bioBERT/relations_{split}.pickle', 'wb') as f:\n",
    "        pickle.dump(predictions_metadata, f)\n",
    "            \n",
    "    return predictions_metadata\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions_metadata, tav=False, threshold=-0.98, print_info=False):\n",
    "    if not tav:\n",
    "        spans_pred = predictions_metadata['spans_greedy']\n",
    "    else:\n",
    "        spans_pred = []\n",
    "        for s_diff, span_tav in zip(predictions_metadata['s_diffs'], predictions_metadata['spans_tav']):\n",
    "            if s_diff < threshold:\n",
    "                spans_pred.append(span_tav)\n",
    "            else:\n",
    "                spans_pred.append((0, 0))\n",
    "                \n",
    "    em = compute_exact_match(spans_pred, predictions_metadata['truth'])\n",
    "    f1 = compute_f1(spans_pred,  predictions_metadata['truth'], predictions_metadata['input_id_list'])\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'EM: {round(em*100, 1)}, F1: {round(f1*100, 1)}')\n",
    "        \n",
    "    return em, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5897f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 64\n",
    "test_dataset = ermQA('relations_qa_bio_test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "train_dataset = ermQA('relations_qa_bio_train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = torch.load('models/bioBERT/r_1_uf_e_2_vl_0.4504.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f754b814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/417 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "100%|██████████| 417/417 [38:38<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "test_predictions_metadata = generate_predictions(model, test_loader, 'test')\n",
    "# with open('predictions/bioBERT/medication_test.pickle', 'rb') as f:\n",
    "#     test_predictions_metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e3ab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 46.4, F1: 49.3\n"
     ]
    }
   ],
   "source": [
    "em, f1 = evaluate_predictions(test_predictions_metadata, tav=False, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb5a9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 51.2, F1: 53.1\n"
     ]
    }
   ],
   "source": [
    "em, f1 = evaluate_predictions(test_predictions_metadata, tav=True, threshold=.59, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2417f468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1249 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "100%|██████████| 1249/1249 [1:55:40<00:00,  5.56s/it]\n"
     ]
    }
   ],
   "source": [
    "train_predictions_metadata = generate_predictions(model, train_loader, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e6782d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM: 48.1, F1: 50.9\n"
     ]
    }
   ],
   "source": [
    "em, f1 = evaluate_predictions(train_predictions_metadata, tav=False, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4158b18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_list = []\n",
    "thresholds = []\n",
    "for threshold in np.arange(-1, 1, .01):\n",
    "    em, f1 = evaluate_predictions(train_predictions_metadata, tav=True, threshold=threshold, print_info=False)\n",
    "    em_list.append(em)\n",
    "    thresholds.append(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "760335a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5899999999999996\n",
      "EM: 59.6, F1: 60.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5962202753441802, 0.6002993313730656)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=thresholds, y=em_list)\n",
    "max_index = em_list.index(max(em_list))\n",
    "threshold = thresholds[max_index]\n",
    "print(threshold)\n",
    "evaluate_predictions(train_predictions_metadata, tav=True, threshold=threshold, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117185b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
