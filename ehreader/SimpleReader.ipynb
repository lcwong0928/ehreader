{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7f6ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig ,DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b837d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ermQA(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        with open(f\"processed_data/{filename}.pickle\", \"rb\") as f:\n",
    "            self.encodings = pickle.load(f)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb302c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTEncoder(torch.nn.Module):\n",
    "    def __init__(self, frozen=True):\n",
    "        super(DistilBERTEncoder, self).__init__()\n",
    "        self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states = True)\n",
    "        self.encoder.to(device)\n",
    "        if frozen:\n",
    "            self.encoder.requires_grad = False\n",
    "            self.encoder.eval()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.encoder(input_ids, attention_mask = attention_mask)\n",
    "        embedding = output.last_hidden_state # [batch, 512, 3072]\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "class SimpleReader(torch.nn.Module):\n",
    "    def __init__(self, in_features=768, out_features=1):\n",
    "        super(SimpleReader, self).__init__()\n",
    "        self.encoder = DistilBERTEncoder(frozen=False)\n",
    "        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embeddings = self.encoder(input_ids, attention_mask = attention_mask)\n",
    "        embedding_first_token = torch.squeeze(embeddings[:, 0, :], axis = 1) # [batch, 3072]    \n",
    "        linear = self.linear(embedding_first_token) # [batch, 1] \n",
    "        logit = self.sigmoid(linear)  # [batch, 1]   \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e77cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "train_dataset = ermQA('medication_qa_train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "VAL_BATCH_SIZE = 8\n",
    "val_dataset = ermQA('medication_qa_val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f641ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleReader()\n",
    "model.to(device)\n",
    "metadata = dict()\n",
    "\n",
    "if metadata == dict():\n",
    "    START_EPOCH = 0\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "else:\n",
    "    START_EPOCH = metadata['epoch'] + 1\n",
    "    train_loss = metadata['train_loss']\n",
    "    val_loss = metadata['valid_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16491246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4536 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "100%|██████████| 4536/4536 [33:05<00:00,  2.28it/s]\n",
      "  0%|          | 0/1512 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 1512/1512 [03:18<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 0.26788594718321146, val_loss: 0.18841451284858748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/4536 [00:02<32:58,  2.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10856/1014970175.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "sr_loss_func = nn.BCELoss()\n",
    "optim = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCHS):\n",
    "    # Train\n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Forward \n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        sr_out = model(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate sr loss\n",
    "        answerability = (batch['start_positions'] < batch['end_positions']).float().view(-1, 1).to(device)\n",
    "        sr_loss = sr_loss_func(sr_out, answerability)\n",
    "\n",
    "        # Calculate loss and backward\n",
    "        batch_loss.append(sr_loss.item())\n",
    "        optim.zero_grad()\n",
    "        sr_loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    " \n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    current_loss = []\n",
    "    batch_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Forward \n",
    "            input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "            attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "            sr_out = model(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate sr loss\n",
    "            answerability = (batch['start_positions'] < batch['end_positions']).float().view(-1, 1).to(device)\n",
    "            sr_loss = sr_loss_func(sr_out, answerability)\n",
    "\n",
    "            # Calculate loss and backward\n",
    "            batch_loss.append(sr_loss.item())\n",
    "  \n",
    "    val_loss.append(np.mean(batch_loss))\n",
    "    \n",
    "    print(f'Epoch: {epoch}, train_loss: {train_loss[-1]}, val_loss: {val_loss[-1]}')\n",
    "    \n",
    "    model_name = f'models/SimpleReader/m_1_uf_e_{len(val_loss)}_vl_{round(val_loss[-1], 4)}'\n",
    "    metadata = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': val_loss\n",
    "    }\n",
    "  \n",
    "    # Early Stopping\n",
    "    if len(val_loss) > 3:\n",
    "        if val_loss[-1] > val_loss[-2] > val_loss[-3]:\n",
    "            torch.save(model, f'{model_name}.model')\n",
    "            \n",
    "            with open(f'{model_name}_metadata.pickle', 'wb') as f:\n",
    "                pickle.dump(metadata, f)\n",
    "            \n",
    "    # Check point\n",
    "    torch.save(model, f'{model_name}.model') \n",
    "\n",
    "    with open(f'{model_name}_metadata.pickle', 'wb') as f:\n",
    "        pickle.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d1e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
