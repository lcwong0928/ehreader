{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepReader.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyO9XPb6JocPOOv76kQWikMS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGgc7gbn71m8","executionInfo":{"status":"ok","timestamp":1638832226581,"user_tz":300,"elapsed":2774,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"a4c5365e-b28b-4ef2-f994-48753f2906fb"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1OBJi0C76Gs","executionInfo":{"status":"ok","timestamp":1638832230135,"user_tz":300,"elapsed":3557,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"51623590-55e7-4dd7-c0a5-42acbe0e1b7f"},"source":["import pandas as pd\n","import json\n","import numpy as np\n","from collections import Counter\n","import pickle\n","from tqdm import tqdm\n","import seaborn as sns\n","import collections\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig ,DistilBertTokenizerFast, DistilBertForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from transformers import DistilBertModel, DistilBertConfig\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","\n","import pickle\n","import torch.optim as optim\n","\n","from google.colab import drive \n","drive.mount('/content/drive')\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"KtClM0Qr77T7","executionInfo":{"status":"ok","timestamp":1638832230136,"user_tz":300,"elapsed":5,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}}},"source":["class ermQA(torch.utils.data.Dataset):\n","    def __init__(self, filename):\n","        with open(f\"/content/drive/Shareddrives/NLP/EHReader/processed_data/{filename}.pickle\", \"rb\") as f:\n","            self.encodings = pickle.load(f)\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"STpjeb6A7857","executionInfo":{"status":"ok","timestamp":1638832230136,"user_tz":300,"elapsed":5,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}}},"source":["class DistilBERTEncoder(torch.nn.Module):\n","    def __init__(self, frozen=True):\n","        super(DistilBERTEncoder, self).__init__()\n","        self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states = True)\n","        self.encoder.to(device)\n","        if frozen:\n","            self.encoder.requires_grad = False\n","            self.encoder.eval()\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.encoder(input_ids, attention_mask = attention_mask)\n","        embedding = output.last_hidden_state # [batch, 512, 3072]\n","\n","        return embedding\n","    \n","class DeepReader(torch.nn.Module):\n","    def __init__(self, embed_size=768, num_heads=1):\n","        super(DeepReader, self).__init__()\n","        self.encoder = DistilBERTEncoder(frozen=False)\n","        \n","        self.key_linear = nn.Linear(embed_size, embed_size)\n","        self.value_linear = nn.Linear(embed_size, embed_size)\n","        self.query_linear = nn.Linear(embed_size, embed_size)\n","        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads, batch_first=True)\n","\n","        # Feed forward neural network (FFN)\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(embed_size, embed_size),\n","            nn.ReLU(),\n","            nn.Linear(embed_size, 2)\n","        )\n","    \n","    def forward(self, input_ids, attention_mask):\n","        embeddings = self.encoder(input_ids, attention_mask = attention_mask)\n","        \n","        key = self.key_linear(embeddings)\n","        value = self.value_linear(embeddings)\n","        query = self.query_linear(embeddings)\n","        embedding, _ = self.attention(query=query, key=key, value=value)\n","        \n","        ffn_output = self.linear_relu_stack(embedding)\n","        output = nn.functional.softmax(ffn_output, dim=1)\n","\n","        return output"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acq5F6cG8CGg","executionInfo":{"status":"ok","timestamp":1638832238065,"user_tz":300,"elapsed":7934,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}}},"source":["TRAIN_BATCH_SIZE = 32\n","train_dataset = ermQA('medication_qa_train')\n","train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","\n","\n","VAL_BATCH_SIZE = 32\n","val_dataset = ermQA('medication_qa_val')\n","val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xs7knT9K8ESx","executionInfo":{"status":"ok","timestamp":1638832242618,"user_tz":300,"elapsed":4562,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"99923f0a-b7cd-4c5d-9a1b-d84233cda8d5"},"source":["model = DeepReader()\n","model.to(device)\n","metadata = dict()\n","\n","if metadata == dict():\n","    START_EPOCH = 0\n","    train_loss = []\n","    val_loss = []\n","else:\n","    START_EPOCH = metadata['epoch'] + 1\n","    train_loss = metadata['train_loss']\n","    val_loss = metadata['valid_loss']"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":588},"id":"7b-YtxIF8FOc","executionInfo":{"status":"error","timestamp":1638834448401,"user_tz":300,"elapsed":1568520,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"832c8851-c915-4c70-d580-91c4ec83a1af"},"source":["NUM_EPOCHS = 2\n","dr_loss_func = nn.CrossEntropyLoss()\n","optim = AdamW(model.parameters(), lr=3e-5)\n","\n","for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCHS):\n","    # Train\n","    model.train()\n","    batch_loss = []\n","    for batch in tqdm(train_loader):\n","        torch.cuda.empty_cache()\n","        \n","        # Forward \n","        input_ids = torch.tensor(batch['input_ids']).to(device)\n","        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","        dr_out = model(input_ids, attention_mask)\n","\n","        # Calculate dr loss\n","        start_logits, end_logits = dr_out.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1).contiguous()\n","        end_logits = end_logits.squeeze(-1).contiguous()\n","\n","        start_loss = dr_loss_func(start_logits,  batch['start_positions'].to(device))\n","        end_loss = dr_loss_func(end_logits, batch['end_positions'].to(device))\n","        dr_loss = (start_loss + end_loss) / 2\n","\n","        # Calculate loss and backward\n","        batch_loss.append(dr_loss.item())\n","        optim.zero_grad()\n","        dr_loss.backward()\n","        optim.step()\n","\n","    train_loss.append(np.mean(batch_loss))\n","  \n","    # Validation\n","    model.eval()\n","    batch_loss = []\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader):\n","            torch.cuda.empty_cache()\n","\n","            # Forward \n","            input_ids = torch.tensor(batch['input_ids']).to(device)\n","            attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","            dr_out = model(input_ids, attention_mask)\n","\n","            # Calculate dr loss\n","            start_logits, end_logits = dr_out.split(1, dim=-1)\n","            start_logits = start_logits.squeeze(-1).contiguous()\n","            end_logits = end_logits.squeeze(-1).contiguous()\n","\n","            start_loss = dr_loss_func(start_logits,  batch['start_positions'].to(device))\n","            end_loss = dr_loss_func(end_logits, batch['end_positions'].to(device))\n","            dr_loss = (start_loss + end_loss) / 2\n","\n","            # Calculate loss and backward\n","            batch_loss.append(dr_loss.item())\n","\n","    val_loss.append(np.mean(batch_loss))\n","    \n","    print(f'Epoch: {epoch}, train_loss: {train_loss[-1]}, val_loss: {val_loss[-1]}')\n","    \n","    model_name = f'/content/drive/Shareddrives/NLP/EHReader/DeepReader/m_1_uf_e_{len(val_loss)}_vl_{round(val_loss[-1], 4)}'\n","    metadata = {\n","        'epoch': epoch,\n","        'train_loss': train_loss,\n","        'valid_loss': val_loss\n","    }\n","  \n","    # Early Stopping\n","    if len(val_loss) > 3:\n","        if val_loss[-1] > val_loss[-2] > val_loss[-3]:\n","            torch.save(model, f'{model_name}.model')\n","            \n","            with open(f'{model_name}_metadata.pickle', 'wb') as f:\n","                pickle.dump(metadata, f)\n","            \n","    # Check point\n","    torch.save(model, f'{model_name}.model') \n","\n","    with open(f'{model_name}_metadata.pickle', 'wb') as f:\n","        pickle.dump(metadata, f)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1134 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","100%|██████████| 1134/1134 [24:09<00:00,  1.28s/it]\n","  0%|          | 0/378 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","100%|██████████| 378/378 [02:13<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0, train_loss: 6.2094117909722435, val_loss: 5.741673818971745\n"]},{"output_type":"stream","name":"stderr","text":[" 43%|████▎     | 485/1134 [10:20<13:50,  1.28s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-4a3ddc663fbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"23JhO5Kv8i2b"},"source":[""],"execution_count":null,"outputs":[]}]}