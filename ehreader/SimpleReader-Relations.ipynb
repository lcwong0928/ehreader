{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SimpleReader-Relations.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMIRF7pwzM33fsP2tc3Of9C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGgc7gbn71m8","executionInfo":{"status":"ok","timestamp":1638837834506,"user_tz":300,"elapsed":3875,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"56cfafbc-debf-4c74-beb5-2eabb3d5c7aa"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1OBJi0C76Gs","executionInfo":{"status":"ok","timestamp":1638837838830,"user_tz":300,"elapsed":4327,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"31695822-f777-40a8-b204-51645199c5ec"},"source":["import pandas as pd\n","import json\n","import numpy as np\n","from collections import Counter\n","import pickle\n","from tqdm import tqdm\n","import seaborn as sns\n","import collections\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig ,DistilBertTokenizerFast, DistilBertForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from transformers import DistilBertModel, DistilBertConfig\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","\n","import pickle\n","import torch.optim as optim\n","\n","from google.colab import drive \n","drive.mount('/content/drive')\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"KtClM0Qr77T7"},"source":["class ermQA(torch.utils.data.Dataset):\n","    def __init__(self, filename):\n","        with open(f\"/content/drive/Shareddrives/NLP/EHReader/processed_data/{filename}.pickle\", \"rb\") as f:\n","            self.encodings = pickle.load(f)\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STpjeb6A7857"},"source":["class DistilBERTEncoder(torch.nn.Module):\n","    def __init__(self, frozen=True):\n","        super(DistilBERTEncoder, self).__init__()\n","        self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states = True)\n","        self.encoder.to(device)\n","        if frozen:\n","            self.encoder.requires_grad = False\n","            self.encoder.eval()\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.encoder(input_ids, attention_mask = attention_mask)\n","        embedding = output.last_hidden_state # [batch, 512, 3072]\n","\n","        return embedding\n","    \n","    \n","class SimpleReader(torch.nn.Module):\n","    def __init__(self, in_features=768, out_features=1):\n","        super(SimpleReader, self).__init__()\n","        self.encoder = DistilBERTEncoder(frozen=False)\n","        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input_ids, attention_mask):\n","        embeddings = self.encoder(input_ids, attention_mask = attention_mask)\n","        embedding_first_token = torch.squeeze(embeddings[:, 0, :], axis = 1) # [batch, 3072]    \n","        linear = self.linear(embedding_first_token) # [batch, 1] \n","        logit = self.sigmoid(linear)  # [batch, 1]   \n","        return logit"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Acq5F6cG8CGg"},"source":["TRAIN_BATCH_SIZE = 32\n","train_dataset = ermQA('relations_qa_train')\n","train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","\n","\n","VAL_BATCH_SIZE = 32\n","val_dataset = ermQA('relations_qa_val')\n","val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xs7knT9K8ESx","executionInfo":{"status":"ok","timestamp":1638837854018,"user_tz":300,"elapsed":5865,"user":{"displayName":"Lawrence Wong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjR6uDFGuAfj3Lq8qxlFTz6m4EWvLDfD7DVyrxn4Q=s64","userId":"01088651821611722086"}},"outputId":"a92a3823-c7b7-423a-b849-7ada5051e60a"},"source":["model = SimpleReader()\n","model.to(device)\n","metadata = dict()\n","\n","if metadata == dict():\n","    START_EPOCH = 0\n","    train_loss = []\n","    val_loss = []\n","else:\n","    START_EPOCH = metadata['epoch'] + 1\n","    train_loss = metadata['train_loss']\n","    val_loss = metadata['valid_loss']"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgPr_R-0Vp5m","outputId":"470a3638-f0de-496b-8830-add9d3d4e8b3"},"source":["NUM_EPOCHS = 2\n","sr_loss_func = nn.BCELoss()\n","optim = AdamW(model.parameters(), lr=3e-5)\n","\n","for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCHS):\n","    # Train\n","    model.train()\n","    batch_loss = []\n","    for batch in tqdm(train_loader):\n","        torch.cuda.empty_cache()\n","        \n","        # Forward \n","        input_ids = torch.tensor(batch['input_ids']).to(device)\n","        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","        sr_out = model(input_ids, attention_mask)\n","\n","        # Calculate sr loss\n","        answerability = (batch['start_positions'] < batch['end_positions']).float().view(-1, 1).to(device)\n","        sr_loss = sr_loss_func(sr_out, answerability)\n","\n","        # Calculate loss and backward\n","        batch_loss.append(sr_loss.item())\n","        optim.zero_grad()\n","        sr_loss.backward()\n","        optim.step()\n","        \n","\n","    train_loss.append(np.mean(batch_loss))\n"," \n","        \n","    # Validation\n","    model.eval()\n","    current_loss = []\n","    batch_loss = []\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader):\n","            torch.cuda.empty_cache()\n","\n","            # Forward \n","            input_ids = torch.tensor(batch['input_ids']).to(device)\n","            attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","            sr_out = model(input_ids, attention_mask)\n","\n","            # Calculate sr loss\n","            answerability = (batch['start_positions'] < batch['end_positions']).float().view(-1, 1).to(device)\n","            sr_loss = sr_loss_func(sr_out, answerability)\n","\n","            # Calculate loss and backward\n","            batch_loss.append(sr_loss.item())\n","  \n","    val_loss.append(np.mean(batch_loss))\n","    \n","    print(f'Epoch: {epoch}, train_loss: {train_loss[-1]}, val_loss: {val_loss[-1]}')\n","    \n","    model_name = f'/content/drive/Shareddrives/NLP/EHReader/SimpleReader/r_1_uf_e_{len(val_loss)}_vl_{round(val_loss[-1], 4)}'\n","    metadata = {\n","        'epoch': epoch,\n","        'train_loss': train_loss,\n","        'valid_loss': val_loss\n","    }\n","  \n","    # Early Stopping\n","    if len(val_loss) > 3:\n","        if val_loss[-1] > val_loss[-2] > val_loss[-3]:\n","            torch.save(model, f'{model_name}.model')\n","            \n","            with open(f'{model_name}_metadata.pickle', 'wb') as f:\n","                pickle.dump(metadata, f)\n","            \n","    # Check point\n","    torch.save(model, f'{model_name}.model') \n","\n","    with open(f'{model_name}_metadata.pickle', 'wb') as f:\n","        pickle.dump(metadata, f)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1134 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"," 64%|██████▍   | 728/1134 [14:12<07:56,  1.17s/it]"]}]},{"cell_type":"code","metadata":{"id":"23JhO5Kv8i2b"},"source":[""],"execution_count":null,"outputs":[]}]}