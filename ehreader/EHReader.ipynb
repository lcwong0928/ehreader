{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"name":"EHReader.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cec46e316e70410e985a454bd2559218":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9e2585a426704b23b306d6bf5c976760","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_bad854918d514d8b8001bbdbba878158","IPY_MODEL_bc0e431f4d9b4376996b9019707d4a3c","IPY_MODEL_34ff88ebbf324150bcfee89fbb195b22"]}},"9e2585a426704b23b306d6bf5c976760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bad854918d514d8b8001bbdbba878158":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d0f003e30f4941e09f17df88b2c94f89","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a2d5728a63c4b1fa3dd366c4894a811"}},"bc0e431f4d9b4376996b9019707d4a3c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_75cbc147e663449abc2e36899664c467","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":483,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":483,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_698904684f0b4e7c995047b0f0302ac2"}},"34ff88ebbf324150bcfee89fbb195b22":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f286103d27d4908adf867e950077378","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483/483 [00:00&lt;00:00, 14.2kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6303eed2b569455baa9ee5f74b44e925"}},"d0f003e30f4941e09f17df88b2c94f89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7a2d5728a63c4b1fa3dd366c4894a811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"75cbc147e663449abc2e36899664c467":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"698904684f0b4e7c995047b0f0302ac2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f286103d27d4908adf867e950077378":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6303eed2b569455baa9ee5f74b44e925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f95d626515574753aaf3d321a9095d74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d37aca35c104b59af15fe77a502de9a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8c7489849756450c8de2ff980d1c955b","IPY_MODEL_11c5814755904aab9cffe3d8261122aa","IPY_MODEL_012d2723fcb5415aa65db1ce4fc56c4d"]}},"0d37aca35c104b59af15fe77a502de9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c7489849756450c8de2ff980d1c955b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_62a0cb08468648cab872d3e0136e6d0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3bbd3e9bd1e345e09cd285be39678f3b"}},"11c5814755904aab9cffe3d8261122aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5b693c00e4d04724b99baae74402f42a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87e761b6f5f740ec8d12d47ed44b4d10"}},"012d2723fcb5415aa65db1ce4fc56c4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_128475cdfb9f4ed38c3089edcf513c30","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 256M/256M [00:15&lt;00:00, 47.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b81c988e2ac4b089c71a01d6ef70b72"}},"62a0cb08468648cab872d3e0136e6d0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3bbd3e9bd1e345e09cd285be39678f3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b693c00e4d04724b99baae74402f42a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"87e761b6f5f740ec8d12d47ed44b4d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"128475cdfb9f4ed38c3089edcf513c30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8b81c988e2ac4b089c71a01d6ef70b72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wq7jtAwgaqeu","executionInfo":{"status":"ok","timestamp":1638742401408,"user_tz":300,"elapsed":7265,"user":{"displayName":"Yang Xiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03709122023087024418"}},"outputId":"9fd261a4-a3de-42f9-d829-18ea2ee80d60"},"source":["!pip install transformers"],"id":"wq7jtAwgaqeu","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.2 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 51.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 595 kB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 56.6 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"]}]},{"cell_type":"code","metadata":{"id":"db7f6ecc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638742421873,"user_tz":300,"elapsed":20472,"user":{"displayName":"Yang Xiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03709122023087024418"}},"outputId":"a0c94031-3fcb-4c6e-d0b9-468a2d9c9384"},"source":["import pandas as pd\n","import json\n","import numpy as np\n","from collections import Counter\n","import pickle\n","from tqdm import tqdm\n","import seaborn as sns\n","import collections\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import AdamW\n","from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig ,DistilBertTokenizerFast, DistilBertForQuestionAnswering\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","from transformers import DistilBertModel, DistilBertConfig\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample\n","\n","import pickle\n","import torch.optim as optim\n","\n","from google.colab import drive \n","drive.mount('/content/drive')\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"],"id":"db7f6ecc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"b837d296"},"source":["class ermQA(torch.utils.data.Dataset):\n","    def __init__(self, filename):\n","        with open(f\"/content/drive/Shareddrives/NLP/EHReader/processed_data/{filename}.pickle\", \"rb\") as f:\n","            self.encodings = pickle.load(f)\n","\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","\n","    def __len__(self):\n","        return len(self.encodings.input_ids)"],"id":"b837d296","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"be8285ca"},"source":["class DistilBERTEncoder(torch.nn.Module):\n","    def __init__(self, frozen=True):\n","        super(DistilBERTEncoder, self).__init__()\n","        self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased', output_hidden_states = True)\n","        self.encoder.to(device)\n","        if frozen:\n","            self.encoder.requires_grad = False\n","            self.encoder.eval()\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.encoder(input_ids, attention_mask = attention_mask)\n","        embedding = torch.cat([output[1][i] for i in [-1,-2,-3,-4]], dim=-1) # [batch, 512, 3072]\n","\n","        return embedding\n","    \n","    \n","class SimpleReader(torch.nn.Module):\n","  \n","    def __init__(self, in_features=3072, out_features=1):\n","        super(SimpleReader, self).__init__()\n","        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, embeddings):\n","        # Embedding for the first token ([CLS]) \n","        embedding_first_token = torch.squeeze(embeddings[:, 0, :], axis = 1) # [batch, 3072]    \n","        linear = self.linear(embedding_first_token) # [batch, 1] \n","        logit = self.sigmoid(linear)  # [batch, 1]   \n","        return logit\n","    \n","\n","class DeepReader(torch.nn.Module):\n","    def __init__(self, embed_size, num_heads):\n","        super(DeepReader, self).__init__()\n","        \n","        # Self attention on passage \n","        self.passage_key_linear = nn.Linear(embed_size, embed_size)\n","        self.passage_value_linear = nn.Linear(embed_size, embed_size)\n","        self.passage_query_linear = nn.Linear(embed_size, embed_size)\n","        self.passage_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads, batch_first=True)\n","\n","        # Self attention on question \n","        self.question_key_linear = nn.Linear(embed_size, embed_size)\n","        self.question_value_linear = nn.Linear(embed_size, embed_size)\n","        self.question_query_linear = nn.Linear(embed_size, embed_size)\n","        self.question_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads, batch_first=True)\n","\n","        # Cross attention \n","        self.cross_query_linear = nn.Linear(embed_size, embed_size)\n","        self.cross_key_linear = nn.Linear(embed_size, embed_size)\n","        self.cross_value_linear = nn.Linear(embed_size, embed_size)\n","        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads, batch_first=True)\n","\n","        # Feed forward neural network (FFN)\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(embed_size, embed_size),\n","            nn.ReLU(),\n","            nn.Linear(embed_size, 2)\n","        )\n","        \n","    \n","    def forward(self, embeddings, token_split_index):\n","        passage, question = torch.tensor_split(embeddings, token_split_index, dim=1)\n","\n","        passage_key = self.passage_key_linear(passage)\n","        passage_value = self.passage_value_linear(passage)\n","        passage_query = self.passage_query_linear(passage)\n","        passage_after_attention, _ = self.passage_attention(query=passage_query, key=passage_key, value=passage_value)\n","\n","        question_key = self.question_key_linear(question)\n","        question_value = self.question_value_linear(question)\n","        question_query = self.question_query_linear(question)\n","        question_after_attention, _ = self.question_attention(query=question_query, key=question_key, value=question_value)\n","\n","        cross_query = self.cross_query_linear(passage_after_attention)\n","        cross_key = self.cross_key_linear(question_after_attention)\n","        cross_value = self.cross_value_linear(question_after_attention)\n","        cross_attention_embedding, _ = self.cross_attention(query=cross_query, key=cross_key, value=cross_value)\n","\n","        ffn_output = self.linear_relu_stack(cross_attention_embedding)\n","        output = nn.functional.softmax(ffn_output, dim=1)\n","\n","        return output, passage\n","\n","    \n","class EHReader(torch.nn.Module):\n","    def __init__(self):\n","        super(EHReader, self).__init__()\n","        self.encoder = DistilBERTEncoder(frozen=False)\n","        self.sr = SimpleReader(in_features=3072, out_features=1)\n","        self.dr = DeepReader(embed_size=3072, num_heads=1)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        embedding = self.encoder(input_ids, attention_mask = attention_mask)\n","        \n","        sr_logits = self.sr(embedding)\n","        \n","        \n","        token_split_index = self.generate_token_split_index(input_ids)\n","        dr_logits, passage = self.dr(embedding, token_split_index)\n","        \n","        return sr_logits, dr_logits, passage\n","    \n","    def generate_token_split_index(self, input_ids):\n","        token_split_index = []\n","        sep_tokens = (input_ids == 102).nonzero(as_tuple=True)\n","        used_samples = set()\n","        for i, index in zip(sep_tokens[0], sep_tokens[1]):\n","            if i.item() not in used_samples:\n","                token_split_index.append(index.item())\n","                used_samples.add(i.item())\n","        return token_split_index"],"id":"be8285ca","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e77cfc5"},"source":["TRAIN_BATCH_SIZE = 1\n","train_dataset = ermQA('relations_qa_train')\n","train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","\n","\n","VAL_BATCH_SIZE = 1\n","val_dataset = ermQA('relations_qa_val')\n","val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=True)"],"id":"1e77cfc5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f641ef6","colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["cec46e316e70410e985a454bd2559218","9e2585a426704b23b306d6bf5c976760","bad854918d514d8b8001bbdbba878158","bc0e431f4d9b4376996b9019707d4a3c","34ff88ebbf324150bcfee89fbb195b22","d0f003e30f4941e09f17df88b2c94f89","7a2d5728a63c4b1fa3dd366c4894a811","75cbc147e663449abc2e36899664c467","698904684f0b4e7c995047b0f0302ac2","9f286103d27d4908adf867e950077378","6303eed2b569455baa9ee5f74b44e925","f95d626515574753aaf3d321a9095d74","0d37aca35c104b59af15fe77a502de9a","8c7489849756450c8de2ff980d1c955b","11c5814755904aab9cffe3d8261122aa","012d2723fcb5415aa65db1ce4fc56c4d","62a0cb08468648cab872d3e0136e6d0b","3bbd3e9bd1e345e09cd285be39678f3b","5b693c00e4d04724b99baae74402f42a","87e761b6f5f740ec8d12d47ed44b4d10","128475cdfb9f4ed38c3089edcf513c30","8b81c988e2ac4b089c71a01d6ef70b72"]},"executionInfo":{"status":"ok","timestamp":1638742499991,"user_tz":300,"elapsed":29897,"user":{"displayName":"Yang Xiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03709122023087024418"}},"outputId":"ece65f2b-6ac1-4cb4-d701-c663b7f5c7d9"},"source":["model = EHReader()\n","\n","# model = torch.load('models/EHReader/m_1_f_e_2_vl_2.9071.model')\n","# with open('models/EHReader/m_1_f_e_2_vl_2.9071.pickle', 'rb') as f:\n","#     metadata = pickle.load(f)\n","    \n","model.to(device)\n","metadata = dict()"],"id":"4f641ef6","execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cec46e316e70410e985a454bd2559218","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f95d626515574753aaf3d321a9095d74","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","metadata":{"id":"16491246","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7a5f2cf6-b79a-4f81-ff73-b5bf754dcc07"},"source":["torch.cuda.empty_cache()\n","\n","NUM_EPOCHS = 20\n","\n","sr_loss_func = nn.BCELoss()\n","dr_loss_func = nn.CrossEntropyLoss()\n","# optimizer = optim.SGD(model.parameters(), lr=1, momentum=0.9)\n","\n","optim = AdamW(model.parameters(), lr=5e-5)\n","\n","if metadata == {}:\n","    START_EPOCH = 0\n","    train_loss = []\n","    val_loss = []\n","else:\n","    START_EPOCH = metadata['epoch'] + 1\n","    train_loss = metadata['train_loss']\n","    val_loss = metadata['valid_loss']\n","\n","\n","for epoch in range(START_EPOCH, START_EPOCH+NUM_EPOCHS):\n","    batch_loss = []\n","    \n","    # Train\n","    model.train()\n","    for batch in tqdm(train_loader):\n","        torch.cuda.empty_cache()\n","        \n","        # Forward \n","        input_ids = torch.tensor(batch['input_ids']).to(device)\n","        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","        sr_out, dr_out, passage_embedding = model(input_ids, attention_mask)\n","\n","        # Calculate loss\n","        answerability = (batch['start_positions'] > batch['end_positions']).float().reshape(-1,1).to(device)\n","        sr_loss = sr_loss_func(sr_out, answerability)\n","        \n","        target = torch.zeros(tuple(passage_embedding.shape[0:2]) + (2, )).to(device)\n","        start_pos = batch['start_positions'][0].item()\n","        end_pos = batch['end_positions'][0].item()\n","        target[0, start_pos, 0] = 1\n","        target[0, end_pos, 1] = 1\n","        dr_loss = dr_loss_func(dr_out, target)\n","        \n","        total_loss = .5 * sr_loss + .5 * dr_loss\n","\n","\n","        # Calculate loss and backward\n","        batch_loss.append(total_loss.item())\n","        optim.zero_grad()\n","        total_loss.backward()\n","        optim.step()\n","\n","    train_loss.append(np.mean(batch_loss))\n"," \n","        \n","    # Validation\n","    model.eval()\n","    batch_loss = []\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader):\n","            torch.cuda.empty_cache()\n","\n","            # Forward \n","            input_ids = torch.tensor(batch['input_ids']).to(device)\n","            attention_mask = torch.tensor(batch['attention_mask']).to(device)\n","            sr_out, dr_out, passage_embedding = model(input_ids, attention_mask)\n","\n","            # Calculate loss\n","            answerability = (batch['start_positions'] > batch['end_positions']).float().reshape(-1,1).to(device)\n","            sr_loss = sr_loss_func(sr_out, answerability)\n","\n","            target = torch.zeros(tuple(passage_embedding.shape[0:2]) + (2, )).to(device)\n","            start_pos = batch['start_positions'][0].item()\n","            end_pos = batch['end_positions'][0].item()\n","            target[0, start_pos, 0] = 1\n","            target[0, end_pos, 1] = 1\n","            dr_loss = dr_loss_func(dr_out, target)\n","\n","            total_loss = .5 * sr_loss + .5 * dr_loss\n","\n","            # Calculate loss and backward\n","            batch_loss.append(total_loss.item())\n","\n","    val_loss.append(np.mean(batch_loss))\n","    \n","    print(f'Epoch: {epoch}, train_loss: {train_loss[-1]}, val_loss: {val_loss[-1]}')\n","    \n","    model_name = f'r_1_uf_e_{len(val_loss)}_vl_{round(val_loss[-1], 4)}'\n","    metadata = {\n","        'epoch': epoch,\n","        'train_loss': train_loss,\n","        'valid_loss': val_loss\n","    }\n","  \n","    # Early Stopping\n","    if len(val_loss) > 3:\n","        if val_loss[-1] > val_loss[-2] > val_loss[-3]:\n","            torch.save(model, f'/content/drive/Shareddrives/NLP/EHReader/model/{model_name}.model')\n","            \n","            with open(f'/content/drive/Shareddrives/NLP/EHReader/model/{model_name}_metadata.pickle', 'wb') as f:\n","                pickle.dump(metadata, f)\n","            break\n","            \n","    # Check point\n","    torch.save(model, f'/content/drive/Shareddrives/NLP/EHReader/model/{model_name}.model ') \n","\n","    with open(f'/content/drive/Shareddrives/NLP/EHReader/model/{model_name}_metadata.pickle', 'wb') as f:\n","        pickle.dump(metadata, f)"],"id":"16491246","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/79900 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"," 16%|█▌        | 12497/79900 [32:13<2:53:11,  6.49it/s]"]}]},{"cell_type":"markdown","metadata":{"id":"324ce2d7"},"source":["# Inference"],"id":"324ce2d7"},{"cell_type":"code","metadata":{"id":"c0ec95d8"},"source":["def compute_exact_match(prediction, truth):\n","    exact_match = 0\n","    for pred, tru in zip(prediction, truth):\n","        if pred == tru:\n","            exact_match += 1\n","    return exact_match / len(prediction)\n","\n","\n","def getOverlap(a, b):\n","    return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n","\n","\n","def compute_tav_fast(start_logits, end_logits):\n","    start_probs, end_probs = (torch.softmax(start_logits, dim=0), torch.softmax(end_logits, dim=0))\n","    n = len(start_logits)\n","    s_null = start_probs[0] + end_probs[0]\n","    s_has = 0\n","    best_span = (0, 0)\n","    high_start_idx = 1\n","  \n","    for i in range(1, n):\n","        if start_probs[i] > start_probs[high_start_idx]:\n","            high_start_idx = i\n","        if start_probs[high_start_idx] + end_probs[i] > s_has:\n","            s_has = start_probs[high_start_idx] + end_probs[i]\n","            best_span = (high_start_idx, i)\n","            \n","    s_diff = s_null - s_has\n","    return s_diff.item(), best_span\n","\n","def compute_f1(prediction, truth, input_id_list):\n","    \n","    f1_scores = []\n","    \n","    for i, (pred, gold) in enumerate(zip(prediction, truth)):\n","        \n","        if pred == gold:\n","            f1_scores.append(1)\n","            \n","        else:\n","            pred_tokens = input_id_list[i-1][pred[0]: pred[1]+1]\n","            gold_tokens = input_id_list[i-1][gold[0]: gold[1]+1]\n","\n","            total_overlap = len(set(pred_tokens).intersection(set(gold_tokens)))\n","            total_tru = len(gold_tokens)\n","            total_pred = len(pred_tokens)\n","\n","            if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n","                f1_scores.append(int(pred_tokens == gold_tokens))\n","            elif total_overlap == 0:\n","                f1_scores.append(0)\n","            else:\n","                prec = total_overlap / total_pred\n","                rec = total_overlap / total_tru\n","                f1_scores.append(2 * (prec * rec) / (prec + rec))\n","    \n","    return np.mean(f1_scores)"],"id":"c0ec95d8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1727f7a"},"source":["def generate_predictions(model, data_loader, split):\n","    truths = []\n","    \n","    # Greedy\n","    spans_greedy = []\n","    \n","    # TAV\n","    s_diffs = []\n","    spans_tav = []\n","    \n","    input_id_list = []\n","    \n","    model.eval()\n","    for batch in tqdm(data_loader):\n","        \n","        # we don't need to calculate gradients as we're not training\n","        with torch.no_grad():\n","            # Truth Span\n","            for i, j, k in zip(batch['start_positions'], batch['end_positions'], batch['input_ids']):\n","                truths.append((i.item(), j.item()))\n","                input_id_list.append(k.tolist())\n","        \n","            \n","            # Predictions\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            \n","            # Greedy\n","            for i, j in zip(torch.argmax(outputs['start_logits'], dim=1), torch.argmax(outputs['end_logits'], dim=1)):\n","                spans_greedy.append((i.item(), j.item()))\n","            \n","            # TAV\n","            for i, j in zip(outputs['start_logits'], outputs['end_logits']):\n","                s_diff, span = compute_tav_fast(i, j)\n","                s_diffs.append(s_diff)\n","                spans_tav.append(span)\n","    \n","    predictions_metadata = {\n","        'truth': truths,\n","        'spans_greedy': spans_greedy,\n","        's_diffs': s_diffs,\n","        'spans_tav': spans_tav,\n","        'input_id_list': input_id_list\n","    }\n","    with open(f'predictions/EHReader/medication_{split}.pickle', 'wb') as f:\n","        pickle.dump(predictions_metadata, f)\n","            \n","    return predictions_metadata\n","\n","\n","def evaluate_predictions(predictions_metadata, tav=False, threshold=-0.98, print_info=False):\n","    if not tav:\n","        spans_pred = predictions_metadata['spans_greedy']\n","    else:\n","        spans_pred = []\n","        for s_diff, span_tav in zip(predictions_metadata['s_diffs'], predictions_metadata['spans_tav']):\n","            if s_diff < threshold:\n","                spans_pred.append(span_tav)\n","            else:\n","                spans_pred.append((0, 0))\n","                \n","    em = compute_exact_match(spans_pred, predictions_metadata['truth'])\n","    f1 = compute_f1(spans_pred,  predictions_metadata['truth'], predictions_metadata['input_id_list'])\n","    \n","    if print_info:\n","        print(f'EM: {round(em*100, 1)}, F1: {round(f1*100, 1)}')\n","        \n","    return em, f1"],"id":"c1727f7a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5b4e2024"},"source":["TEST_BATCH_SIZE = 1\n","test_dataset = ermQA('medication_qa_test')\n","test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n","\n","train_dataset = ermQA('medication_qa_train')\n","train_loader = DataLoader(train_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False)\n","\n","model = torch.load('models/EHReader/m_1_f_e_2_vl_2.9071.model')\n","with open('models/EHReader/m_1_f_e_2_vl_2.9071_metadata.pickle', 'rb') as f:\n","    metadata = pickle.load(f)"],"id":"5b4e2024","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7f05cba","outputId":"1d2f9125-af6c-4a21-9297-4d505aa6dece"},"source":["train_predictions_metadata = generate_predictions(model, train_loader, 'test')"],"id":"a7f05cba","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/36288 [00:00<?, ?it/s]C:\\Users\\lcwon\\Anaconda3\\envs\\gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","  0%|          | 0/36288 [00:01<?, ?it/s]\n"]},{"ename":"TypeError","evalue":"tuple indices must be integers or slices, not str","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7140/4064669106.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_predictions_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7140/1509076920.py\u001b[0m in \u001b[0;36mgenerate_predictions\u001b[1;34m(model, data_loader, split)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# Greedy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mspans_greedy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"]}]},{"cell_type":"code","metadata":{"id":"ef3f7eb8"},"source":["em, f1 = evaluate_predictions(test_predictions_metadata, tav=False, print_info=True)"],"id":"ef3f7eb8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5be81af"},"source":["em, f1 = evaluate_predictions(test_predictions_metadata, tav=True, threshold=-0.65, print_info=True)"],"id":"d5be81af","execution_count":null,"outputs":[]}]}